{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38cfc4ed-afbf-4aaa-b840-7a9718695b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import order_independent_llm\n",
    "import order_independent_llm.plot_helpers\n",
    "import json\n",
    "import seaborn as sns\n",
    "from functools import lru_cache\n",
    "import multiprocessing\n",
    "\n",
    "# Base paths\n",
    "BASE_DIR = \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp\"\n",
    "RESULTS_DIR_MMLU = \"results/mmlu_quoted\"\n",
    "RESULTS_DIR_CSQA = \"results/csqa_quoted\"\n",
    "OUTPUT_DIR = \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/20250105-005619_tags-False\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad150afd-3ab9-4dd2-9500-831105688600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model pairs (pre and post finetuning)\n",
    "model_pairs = [\n",
    "    {\n",
    "        \"name\": \"Llama-2-7b\",\n",
    "        \"pre_path\": \"meta-llama_Llama-2-7b-hf-50\",\n",
    "        \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/20250105-005619_tags-False\", # stores benchmarks.jsonl\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Llama-2-7b : QA\",\n",
    "        \"pre_path\": \"meta-llama_Llama-2-7b-hf-50\",\n",
    "        \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa/20250106-022456-False\", # stores benchmarks.jsonl\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "15fe8c0f-2231-493b-8595-d89d7fa9cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model pairs (pre and post finetuning)\n",
    "# NOTE: all trained with 2 epochs (I want to switch back to 3)\n",
    "model_pairs = [\n",
    "    {\n",
    "        \"name\": \"Llama-2-7b : QA\",\n",
    "        \"pre_path\": \"meta-llama_Llama-2-7b-hf-50\",\n",
    "        \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa/20250109-090124-False\", # stores benchmarks.jsonl\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Llama-2-7b : QA+wiki\",\n",
    "        \"pre_path\": \"meta-llama_Llama-2-7b-hf-50\",\n",
    "        \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa_wiki/20250109-092335-False\", # stores benchmarks.jsonl\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Llama-2-7b : Q only\",\n",
    "        \"pre_path\": \"meta-llama_Llama-2-7b-hf-50\",\n",
    "        \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/20250105-005619_tags-False\", # stores benchmarks.jsonl\n",
    "    },\n",
    "    #{\n",
    "    #    \"name\": \"Llama-2-7b : QA (Prev)\",\n",
    "    #    \"pre_path\": \"meta-llama_Llama-2-7b-hf-50\",\n",
    "    #    \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa/20250106-022456-False\", # stores benchmarks.jsonl\n",
    "    #},\n",
    "    {\n",
    "        \"name\": \"Llama-2-7b : QA+s2d\",\n",
    "        \"pre_path\": \"meta-llama_Llama-2-7b-hf-50\",\n",
    "        \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa_s2d/20250109-094716-False\", # stores benchmarks.jsonl\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fe5e8b29-b81c-4988-a5e2-6080cf91a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 models for comparison\n",
    "model_pairs = [\n",
    "    {\n",
    "        \"name\": \"Llama-2-7b : QA+wiki\",\n",
    "        \"pre_path\": \"meta-llama_Llama-2-7b-hf-50\",\n",
    "        \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa_wiki/20250109-092335-False\", # stores benchmarks.jsonl\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Llama-2-7b-chat : QA+wiki\",\n",
    "        \"pre_path\": \"meta-llama_Llama-2-7b-chat-hf-50\",\n",
    "        \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-chat-hf/mmlu_quoted_qa_wiki/20250110-001813-False\", # stores benchmarks.jsonl\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Llama-2-13b : QA+wiki\",\n",
    "        \"pre_path\": \"meta-llama_Llama-2-13b-hf-50\",\n",
    "        \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-13b-hf/mmlu_quoted_qa_wiki/20250110-002528-False\", # stores benchmarks.jsonl\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Llama-2-13b-chat : QA+wiki\",\n",
    "        \"pre_path\": \"meta-llama_Llama-2-13b-chat-hf-50\",\n",
    "        \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-13b-chat-hf/mmlu_quoted_qa_wiki/20250109-235941-False\", # stores benchmarks.jsonl\n",
    "    },\n",
    "]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6dc68407-fb42-4468-a39f-1e448a77693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-2-7b, trained on either MMLU or CSQA\n",
    "model_pairs = [\n",
    "    {\n",
    "        \"name\": \"Llama-2-7b : QA+wiki\",\n",
    "        \"pre_path\": \"meta-llama_Llama-2-7b-hf-50\",\n",
    "        \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa_wiki/20250109-092335-False\", # stores benchmarks.jsonl\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Llama-2-7b : QA+wiki : train_csqa\",\n",
    "        \"pre_path\": \"meta-llama_Llama-2-7b-hf-50\",\n",
    "        \"output_dir\": \"/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/csqa_quoted_qa_wiki/20250112-040235-False\", # stores benchmarks.jsonl\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3844e5fb-016c-4c7f-a88b-c3bd84377ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa/20250109-090124-False\n",
      "/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa_wiki/20250109-092335-False\n",
      "/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/20250105-005619_tags-False\n",
      "/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa_s2d/20250109-094716-False\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model_pairs)):\n",
    "    print(model_pairs[i][\"output_dir\"])\n",
    "    if \"s2d\" not in model_pairs[i][\"output_dir\"]:\n",
    "        model_pairs[i][\"post_path_csqa\"] = f\"results/csqa_quoted/\"+model_pairs[i][\"output_dir\"].replace(\"/\",\"_\")+\"_final_weights-50\"\n",
    "        model_pairs[i][\"post_path_mmlu\"] = f\"results/mmlu_quoted/\"+model_pairs[i][\"output_dir\"].replace(\"/\",\"_\")+\"_final_weights-50\"\n",
    "    else:\n",
    "        model_pairs[i][\"post_path_csqa\"] = f\"results/csqa_quoted_s2d/\"+model_pairs[i][\"output_dir\"].replace(\"/\",\"_\")+\"_final_weights-50\"\n",
    "        model_pairs[i][\"post_path_mmlu\"] = f\"results/mmlu_quoted_s2d/\"+model_pairs[i][\"output_dir\"].replace(\"/\",\"_\")+\"_final_weights-50\"\n",
    "    model_pairs[i][\"pre_path_csqa\"] = f\"results/csqa_quoted/\"+model_pairs[i][\"pre_path\"]\n",
    "    model_pairs[i][\"pre_path_mmlu\"] = f\"results/mmlu_quoted/\"+model_pairs[i][\"pre_path\"]\n",
    "    model_pairs[i][\"pre_path_csqa_perm\"] = f\"results/csqa_quoted_permutations/\"+model_pairs[i][\"pre_path\"]\n",
    "    model_pairs[i][\"pre_path_mmlu_perm\"] = f\"results/mmlu_quoted_permutations/\"+model_pairs[i][\"pre_path\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3a0db5c7-6d83-4263-b1c3-8845a5d95e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Llama-2-7b after 1 epoch of training (as comparison to see if catastrophic forgetting is reduced)\n",
    "model_pairs.append({k:v for k,v in model_pairs[0].items()})\n",
    "model_pairs[-1][\"name\"] = \"Llama-2-7b : QA+wiki (1 epoch)\"\n",
    "model_pairs[-1][\"post_path_csqa\"] = model_pairs[-1][\"post_path_csqa\"].replace(\"final_weights\",\"checkpoint-493\")\n",
    "model_pairs[-1][\"post_path_mmlu\"] = model_pairs[-1][\"post_path_mmlu\"].replace(\"final_weights\",\"checkpoint-493\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8d8709bb-17aa-46ca-bdf1-8a415e7c3e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Llama-2-7b : QA+wiki',\n",
       "  'pre_path': 'meta-llama_Llama-2-7b-hf-50',\n",
       "  'output_dir': '/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa_wiki/20250109-092335-False',\n",
       "  'post_path_csqa': 'results/csqa_quoted/_n_netscratch_dwork_lab_Lab_katrina_finetuning_sbp_meta-llama_Llama-2-7b-hf_mmlu_quoted_qa_wiki_20250109-092335-False_final_weights-50',\n",
       "  'post_path_mmlu': 'results/mmlu_quoted/_n_netscratch_dwork_lab_Lab_katrina_finetuning_sbp_meta-llama_Llama-2-7b-hf_mmlu_quoted_qa_wiki_20250109-092335-False_final_weights-50',\n",
       "  'pre_path_csqa': 'results/csqa_quoted/meta-llama_Llama-2-7b-hf-50',\n",
       "  'pre_path_mmlu': 'results/mmlu_quoted/meta-llama_Llama-2-7b-hf-50',\n",
       "  'pre_path_csqa_perm': 'results/csqa_quoted_permutations/meta-llama_Llama-2-7b-hf-50',\n",
       "  'pre_path_mmlu_perm': 'results/mmlu_quoted_permutations/meta-llama_Llama-2-7b-hf-50'},\n",
       " {'name': 'Llama-2-7b : QA+wiki : train_csqa',\n",
       "  'pre_path': 'meta-llama_Llama-2-7b-hf-50',\n",
       "  'output_dir': '/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/csqa_quoted_qa_wiki/20250112-040235-False',\n",
       "  'post_path_csqa': 'results/csqa_quoted/_n_netscratch_dwork_lab_Lab_katrina_finetuning_sbp_meta-llama_Llama-2-7b-hf_csqa_quoted_qa_wiki_20250112-040235-False_final_weights-50',\n",
       "  'post_path_mmlu': 'results/mmlu_quoted/_n_netscratch_dwork_lab_Lab_katrina_finetuning_sbp_meta-llama_Llama-2-7b-hf_csqa_quoted_qa_wiki_20250112-040235-False_final_weights-50',\n",
       "  'pre_path_csqa': 'results/csqa_quoted/meta-llama_Llama-2-7b-hf-50',\n",
       "  'pre_path_mmlu': 'results/mmlu_quoted/meta-llama_Llama-2-7b-hf-50',\n",
       "  'pre_path_csqa_perm': 'results/csqa_quoted_permutations/meta-llama_Llama-2-7b-hf-50',\n",
       "  'pre_path_mmlu_perm': 'results/mmlu_quoted_permutations/meta-llama_Llama-2-7b-hf-50'},\n",
       " {'name': 'Llama-2-7b : QA+wiki (1 epoch)',\n",
       "  'pre_path': 'meta-llama_Llama-2-7b-hf-50',\n",
       "  'output_dir': '/n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa_wiki/20250109-092335-False',\n",
       "  'post_path_csqa': 'results/csqa_quoted/_n_netscratch_dwork_lab_Lab_katrina_finetuning_sbp_meta-llama_Llama-2-7b-hf_mmlu_quoted_qa_wiki_20250109-092335-False_checkpoint-493-50',\n",
       "  'post_path_mmlu': 'results/mmlu_quoted/_n_netscratch_dwork_lab_Lab_katrina_finetuning_sbp_meta-llama_Llama-2-7b-hf_mmlu_quoted_qa_wiki_20250109-092335-False_checkpoint-493-50',\n",
       "  'pre_path_csqa': 'results/csqa_quoted/meta-llama_Llama-2-7b-hf-50',\n",
       "  'pre_path_mmlu': 'results/mmlu_quoted/meta-llama_Llama-2-7b-hf-50',\n",
       "  'pre_path_csqa_perm': 'results/csqa_quoted_permutations/meta-llama_Llama-2-7b-hf-50',\n",
       "  'pre_path_mmlu_perm': 'results/mmlu_quoted_permutations/meta-llama_Llama-2-7b-hf-50'}]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a109d440-1109-44d1-8cf4-92e1c2a8e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/csqa_quoted/_n_netscratch_dwork_lab_Lab_katrina_finetuning_sbp_meta-llama_Llama-2-7b-hf_mmlu_quoted_qa_wiki_20250109-092335-False_checkpoint-493-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ea95b-5122-4565-8259-1f974c57913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "@lru_cache\n",
    "def load_results(model_path):\n",
    "    \"\"\"Load results for a given model and dataset type (MMLU or CSQA)\"\"\"\n",
    "    files = glob.glob(f\"*{model_path}/*.jsonl\")\n",
    "    \n",
    "    if not files:\n",
    "        model_path = model_path.replace(\"final_weights-50\",\"final_weights_-50\")\n",
    "        files = glob.glob(f\"*{model_path}/*.jsonl\")\n",
    "    if not files:\n",
    "        model_path = \"/n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/\"+model_path\n",
    "        files = glob.glob(f\"{model_path}/*.jsonl\")\n",
    "    if not files:\n",
    "        model_path = model_path.replace(\"final_weights_-50\",\"final_weights-50\")\n",
    "        files = glob.glob(f\"{model_path}/*.jsonl\")\n",
    "    if not files:\n",
    "        print(f\"No results found for {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    #df = pd.concat([order_independent_llm.load_to_dataframe(f, fail_on_empty=False) for f in files])\n",
    "    def load(x):\n",
    "        return order_independent_llm.load_to_dataframe(x,fail_on_empty=False, include_probs=False)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        dataframes = list(executor.map(load, files))\n",
    "    \n",
    "    # Combine all dataframes into one\n",
    "    df = pd.concat(dataframes)\n",
    "    print(f\"Loading {df.shape} results from {model_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2daffeae-1368-4de3-9740-c4a01a3dc374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_permutation_boxplot(model_results, dataset_type, output_name, suffix=\"\"):\n",
    "    \"\"\"Create boxplot showing distribution of accuracies across permutations for all models\"\"\"\n",
    "    df = pd.concat([model_info['pre_data_perm'].assign(model=model_info['name']) for model_info in model_results])#.drop_duplicates()\n",
    "    df_post = pd.concat([model_info['post_data'].assign(model=model_info['name']) for model_info in model_results])#.drop_duplicates()\n",
    "    '''\n",
    "    'response_type', 'model', 'max_new_tokens', 'order_independent_output',\n",
    "       'pad_attention', 'text_output', 'is_correct_answer',\n",
    "       'correct_answer_prob', 'raw_output_contains_correct_answer_only',\n",
    "       'edit_position', 'edit_attention', 'label_scores', 'label_perplexities',\n",
    "       'probs', 'meta_label', 'meta_incorrect_answers', 'prompt', 'file_name'\n",
    "    '''\n",
    "    #df_post['model'] = \"meta-llama/\" + df_post['model']\n",
    "    #df['model'] = \"meta-llama/\" + df['model']\n",
    "\n",
    "    # only include rows with matching file_name and prompt\n",
    "    common_keys = set(df['prompt']) & set(df_post['prompt'])\n",
    "    df = df[df['prompt'].isin(common_keys)]\n",
    "    df_post = df_post[df_post['prompt'].isin(common_keys) & (df_post['response_type'] == 'order_independent')]\n",
    "    print(f\"After filtering to only create permutation boxplot from common rows: df_post.shape={df_post.shape}\")\n",
    "    print(df_post[\"model\"].value_counts())\n",
    "    df = df[df[\"model\"].isin(df_post[\"model\"].unique())]\n",
    "    #print(df['model'].value_counts())\n",
    "    perms = list(df['response_type'].unique())[3:]\n",
    "    \n",
    "    # 1. Gather all y-values in one array/Series:\n",
    "    df_means       = df[df['response_type'].isin(perms)]\\\n",
    "                        [['model','is_correct_answer','response_type']]\\\n",
    "                        .groupby(['model','response_type'])\\\n",
    "                        .mean()['is_correct_answer']\n",
    "    df_post_means  = df_post[df_post['response_type'] == 'order_independent']\\\n",
    "                        [['model','is_correct_answer','response_type']]\\\n",
    "                        .groupby(['model','response_type'])\\\n",
    "                        .mean()['is_correct_answer']\n",
    "    df_pre_means   = df[df['response_type'] == 'order_independent']\\\n",
    "                        [['model','is_correct_answer','response_type']]\\\n",
    "                        .groupby(['model','response_type'])\\\n",
    "                        .mean()['is_correct_answer']\n",
    "    \n",
    "    all_y_values = pd.concat([df_means, df_post_means, df_pre_means])\n",
    "    \n",
    "    # 2. Compute the global min and max of all_y_values:\n",
    "    y_min, y_max = all_y_values.min(), all_y_values.max()\n",
    "    \n",
    "    # 3. Plot:\n",
    "    fig, ax = plt.subplots(figsize = (7,5))\n",
    "    \n",
    "    sns.boxplot(\n",
    "        x='model',\n",
    "        y='is_correct_answer',\n",
    "        data=df_means.reset_index(),\n",
    "        ax=ax,\n",
    "        whis=[0, 100],\n",
    "        width=.6,\n",
    "    )\n",
    "    sns.stripplot(\n",
    "        x='model',\n",
    "        y='is_correct_answer',\n",
    "        data=df_means.reset_index(),\n",
    "        ax=ax,\n",
    "        label='Normal Model',\n",
    "    )\n",
    "    sns.scatterplot(\n",
    "        x='model',\n",
    "        y='is_correct_answer',\n",
    "        data=df_post_means.reset_index(),\n",
    "        ax=ax,\n",
    "        s=100,\n",
    "        label='Post-Finetuning Order Independent Model',\n",
    "        zorder=5,\n",
    "    )\n",
    "    sns.scatterplot(\n",
    "        x='model',\n",
    "        y='is_correct_answer',\n",
    "        data=df_pre_means.reset_index(),\n",
    "        ax=ax,\n",
    "        s=100,\n",
    "        label='Pre-Finetuning Order Independent Model',\n",
    "        zorder=4,\n",
    "    )\n",
    "    \n",
    "    # Now ensure the y-limit covers the entire data range:\n",
    "    ax.set_ylim([y_min-0.03, y_max+0.03])\n",
    "    ax.set_xticklabels([l._text.split('/')[-1] for l in ax.get_xticklabels()], rotation=90, ha='right')\n",
    "    \n",
    "    ax.legend(ax.get_legend_handles_labels()[0][-2:],ax.get_legend_handles_labels()[1][-2:],bbox_to_anchor=(1,1),loc = 'upper left')\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(f\"{dataset_type} Top 1 Accuracy\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/{suffix}/{output_name}_boxplot.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Create DataFrame with Pre-OID and Post-OID columns\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Pre-OID': df_pre_means.reset_index()['is_correct_answer'],\n",
    "        'Post-OID': df_post_means.reset_index()['is_correct_answer'],\n",
    "    })\n",
    "    \n",
    "    # Ensure model is the index\n",
    "    # TODO: double-check that boxplot indexing labels is correct!!!\n",
    "    comparison_df.index.name = 'model'\n",
    "    print(\"set index\",set(df_post[\"model\"].tolist()))\n",
    "    comparison_df.index = [m for m in df_post_means.reset_index()['model'].unique().tolist() if m in set(df_post[\"model\"].tolist())]\n",
    "    \n",
    "    # Print the DataFrame\n",
    "    print(f\"Boxplot {dataset_type} Accuracy:\")\n",
    "    print(comparison_df.round(3))  # Round to 3 decimal places for cleaner output\n",
    "\n",
    "def create_comparison_barplot(model_results, dataset_type, output_name, suffix=\"\"):\n",
    "    \"\"\"Create barplot comparing normal vs order independent accuracies for all models\"\"\"\n",
    "    #print(\"=======Barplot=========\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    plot_data = []\n",
    "    \n",
    "    # Process each model's data\n",
    "    for model_info in model_results:\n",
    "        pre_data = model_info['pre_data']\n",
    "        post_data = model_info['post_data']\n",
    "        #print(\"Pre-filter:\",pre_data.shape,post_data.shape)\n",
    "        common_keys = set(pre_data['prompt']) & set(post_data['prompt'])\n",
    "        pre_data = pre_data[pre_data['prompt'].isin(common_keys)]\n",
    "        post_data = post_data[post_data['prompt'].isin(common_keys) & (post_data['response_type'] == 'order_independent')]\n",
    "        #print(\"Post-filter:\",pre_data.shape,post_data.shape)\n",
    "        print(model_info['name'],post_data.shape[0],\"post datapoints\")\n",
    "        if post_data.shape[0]==0:\n",
    "            continue\n",
    "        model_name = model_info['name']\n",
    "        \n",
    "        if pre_data is None:\n",
    "            continue\n",
    "            \n",
    "        # Pre-finetuning normal accuracy with error bars\n",
    "        df_pivot = pre_data.drop_duplicates(['prompt', 'response_type']).pivot(\n",
    "            index='prompt',\n",
    "            columns='response_type',\n",
    "            values='is_correct_answer'\n",
    "        )[['normal', 'normal_reversed']]\n",
    "        # Error is proportion where normal=False but reversed=True\n",
    "        error = ((~df_pivot['normal']) & (df_pivot['normal_reversed'])).mean()#float(df_mean.loc[(False, True)] if (False, True) in df_mean.index else 0)\n",
    "        \n",
    "        # Normal accuracy is mean of normal responses\n",
    "        normal_acc = pre_data[pre_data['response_type'] == 'normal']['is_correct_answer'].mean()\n",
    "        \n",
    "        plot_data.append({\n",
    "            'Model': model_name,\n",
    "            'Type': 'Normal',\n",
    "            'Accuracy': normal_acc,\n",
    "            'Error': error\n",
    "        })\n",
    "        \n",
    "        # Pre-finetuning order independent\n",
    "        pre_oid = pre_data[pre_data['response_type'] == 'order_independent']['is_correct_answer'].mean()\n",
    "        plot_data.append({\n",
    "            'Model': model_name,\n",
    "            'Type': 'Pre-OID',\n",
    "            'Accuracy': pre_oid,\n",
    "            'Error': 0\n",
    "        })\n",
    "        \n",
    "        # Post-finetuning order independent\n",
    "        if post_data is not None:\n",
    "            post_oid = post_data[post_data['response_type'] == 'order_independent']['is_correct_answer'].mean()\n",
    "            plot_data.append({\n",
    "                'Model': model_name,\n",
    "                'Type': 'Post-OID',\n",
    "                'Accuracy': post_oid,\n",
    "                'Error': 0,\n",
    "            })\n",
    "    \n",
    "    # Create plot\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    # Create pivot table with Models as rows and Types as columns\n",
    "    reshaped_df = plot_df.pivot(index='Model', columns='Type', values='Accuracy')\n",
    "    \n",
    "    # Print the reshaped DataFrame\n",
    "    print(f\"Barplot {dataset_type} Accuracy:\")\n",
    "    print(reshaped_df[[\"Normal\",\"Pre-OID\",\"Post-OID\"]].round(3))  # Round to 3 decimal places for cleaner output\n",
    "    \n",
    "    # Create grouped barplot\n",
    "    sns.barplot(\n",
    "        data=plot_df,\n",
    "        x='Model',\n",
    "        y='Accuracy',\n",
    "        hue='Type',\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # Add error bars\n",
    "    for i, model in enumerate(plot_df['Model'].unique()):\n",
    "        model_data = plot_df[plot_df['Model'] == model]\n",
    "        normal_data = model_data[model_data['Type'] == 'Normal']\n",
    "        if not normal_data.empty:\n",
    "            ax.errorbar(\n",
    "                x=i - 0.2,  # Adjust position to align with correct bar\n",
    "                y=normal_data['Accuracy'].iloc[0],\n",
    "                yerr=normal_data['Error'].iloc[0],\n",
    "                fmt='none',\n",
    "                c='black'\n",
    "            )\n",
    "    \n",
    "    ax.set_title(f\"{dataset_type} Accuracy Comparison - All Models\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Top 1 Accuracy\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(title=\"\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/{suffix}/{output_name}_barplot.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4a0d7c5d-9c8b-43f3-9c04-06fa670a73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_perplexity_results(model_dir):\n",
    "    \"\"\"Load WikiText-103 perplexity results from benchmarks.jsonl\"\"\"\n",
    "    benchmarks_file = os.path.join(model_dir, \"benchmarks.jsonl\")\n",
    "    if not os.path.exists(benchmarks_file):\n",
    "        print(f\"No benchmarks file found at {benchmarks_file}\")\n",
    "        return None\n",
    "        \n",
    "    results = []\n",
    "    with open(benchmarks_file, 'r') as f:\n",
    "        for line in f:\n",
    "            results.append(json.loads(line))\n",
    "            \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Extract initial and final weights perplexity\n",
    "    initial_perp = df[df['model_path'].str.contains('initial_weights')]['wikitext_perplexity'].iloc[0]\n",
    "    final_perp = df[df['model_path'].str.contains('final_weights')]['wikitext_perplexity'].iloc[0]\n",
    "    \n",
    "    return {\n",
    "        'initial_perplexity': initial_perp,\n",
    "        'final_perplexity': final_perp\n",
    "    }\n",
    "\n",
    "def load_loss_data(model_dir):\n",
    "    \"\"\"Load and process training loss data\"\"\"\n",
    "    loss_file = os.path.join(model_dir, \"loss.jsonl\")\n",
    "    if not os.path.exists(loss_file):\n",
    "        print(f\"No loss file found at {loss_file}\")\n",
    "        return None\n",
    "    \n",
    "    # Read loss data\n",
    "    losses = []\n",
    "    with open(loss_file, 'r') as f:\n",
    "        for line in f:\n",
    "            losses.append(json.loads(line))\n",
    "    \n",
    "    # Convert to DataFrame and deduplicate\n",
    "    df = pd.DataFrame(losses).drop_duplicates()\n",
    "    \n",
    "    # Filter and process data\n",
    "    df = df[df['phase'] != 'final']  # Remove final phase\n",
    "    #df.loc[df['phase'] == 'initial', 'epoch'] = -1  # Set initial phase to epoch -1\n",
    "    df.loc[df['phase'] == 'initial', 'epoch'] = 0  # Set initial phase to epoch 0\n",
    "    df.loc[df['phase'] == 'epoch', 'epoch'] = df.loc[df['phase'] == 'epoch', 'epoch'] + 1  # Shift epochs up by 1\n",
    "    df = df.sort_values('epoch')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_perplexity_plot(perplexity_data, suffix=\"\"):\n",
    "    \"\"\"Create barplot comparing initial and final perplexities across models\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    plot_data = []\n",
    "    for model_name, perp in perplexity_data.items():\n",
    "        if perp is not None:\n",
    "            plot_data.extend([\n",
    "                {\n",
    "                    'Model': model_name,\n",
    "                    'Type': 'Initial Weights',\n",
    "                    'Perplexity': perp['initial_perplexity']\n",
    "                },\n",
    "                {\n",
    "                    'Model': model_name,\n",
    "                    'Type': 'Final Weights',\n",
    "                    'Perplexity': perp['final_perplexity']\n",
    "                }\n",
    "            ])\n",
    "    \n",
    "    if not plot_data:\n",
    "        print(\"No perplexity data available\")\n",
    "        return\n",
    "        \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Create grouped barplot\n",
    "    sns.barplot(\n",
    "        data=plot_df,\n",
    "        x='Model',\n",
    "        y='Perplexity',\n",
    "        hue='Type',\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(\"WikiText-103 Perplexity - Finetuning\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Perplexity\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(title=\"\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/{suffix}/finetuning_perplexity.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_loss_plot(model_losses, suffix=\"\"):\n",
    "    \"\"\"Create lineplot of training and validation losses\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for model_name, loss_df in model_losses.items():\n",
    "        if loss_df is not None:\n",
    "            # Plot train loss\n",
    "            ax.plot(loss_df['epoch'], loss_df['train_loss'], \n",
    "                   label=f'{model_name} (train)', \n",
    "                   marker='o')\n",
    "            \n",
    "            # Plot eval loss\n",
    "            ax.plot(loss_df['epoch'], loss_df['eval_loss'], \n",
    "                   label=f'{model_name} (val)', \n",
    "                   marker='o', \n",
    "                   linestyle='--')\n",
    "    \n",
    "    ax.set_title(\"Training and Validation Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/{suffix}/finetuning_loss.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_perplexity(model_pairs, suffix=\"\"):\n",
    "    \"\"\"Analyze perplexity and training loss for all models\"\"\"\n",
    "    # Create plots directory if it doesn't exist\n",
    "    os.makedirs(f\"plots/{suffix}\", exist_ok=True)\n",
    "    \n",
    "    # Collect perplexity data\n",
    "    perplexity_data = {}\n",
    "    model_losses = {}\n",
    "    \n",
    "    for pair in model_pairs:\n",
    "        # Load perplexity data\n",
    "        perp_results = load_perplexity_results(pair[\"output_dir\"])\n",
    "        if perp_results is not None:\n",
    "            perplexity_data[pair[\"name\"]] = perp_results\n",
    "            \n",
    "        # Load loss data\n",
    "        loss_results = load_loss_data(pair[\"output_dir\"])\n",
    "        if loss_results is not None:\n",
    "            model_losses[pair[\"name\"]] = loss_results\n",
    "    \n",
    "    # Create plots\n",
    "    create_perplexity_plot(perplexity_data, suffix)\n",
    "    create_loss_plot(model_losses, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a1daadb3-a278-4744-9303-8a5d906de7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset_type, suffix=\"\"):\n",
    "    \"\"\"Analyze results for either MMLU or CSQA\"\"\"\n",
    "    os.makedirs(f\"plots/{suffix}\", exist_ok=True)\n",
    "    model_results = []\n",
    "    \n",
    "    # Collect all model results first\n",
    "    for pair in model_pairs:\n",
    "        # Load pre-finetuning results\n",
    "        pre_results = load_results(pair[f\"pre_path_{dataset_type.lower()}\"])\n",
    "        if pre_results is None:\n",
    "            continue\n",
    "            \n",
    "        # Load post-finetuning results\n",
    "        post_results = load_results(pair[f\"post_path_{dataset_type.lower()}\"])\n",
    "\n",
    "        pre_results_perm = load_results(pair[f\"pre_path_{dataset_type.lower()}_perm\"])\n",
    "        \n",
    "        model_results.append({\n",
    "            'name': pair['name'],\n",
    "            'pre_data': pre_results,\n",
    "            'post_data': post_results,\n",
    "            'pre_data_perm': pre_results_perm,\n",
    "        })\n",
    "\n",
    "    create_comparison_barplot(\n",
    "        model_results,\n",
    "        dataset_type,\n",
    "        f\"{dataset_type.lower()}\",\n",
    "        suffix,\n",
    "    )\n",
    "    \n",
    "    # Create plots with all models\n",
    "    create_permutation_boxplot(\n",
    "        model_results,\n",
    "        dataset_type,\n",
    "        f\"{dataset_type.lower()}\",\n",
    "        suffix,\n",
    "    )\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3b3f6ebe-4cf9-461b-810c-30a6bc15ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import math\n",
    "\n",
    "def create_image_grid(image_dir, output_path, images_per_row=None):\n",
    "    \"\"\"\n",
    "    Create a grid of images from all PNGs in a directory\n",
    "    \n",
    "    Args:\n",
    "        image_dir: Directory containing PNG images\n",
    "        output_path: Path to save the output grid image\n",
    "        images_per_row: Number of images per row (optional, will calculate square grid if None)\n",
    "    \"\"\"\n",
    "    # Get all PNG files\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith('.png') and \"combined\" not in f]\n",
    "    if not image_files:\n",
    "        print(\"No PNG files found in directory\")\n",
    "        return\n",
    "        \n",
    "    # Open all images\n",
    "    images = [Image.open(os.path.join(image_dir, f)) for f in image_files]\n",
    "    \n",
    "    # Calculate grid size\n",
    "    n_images = len(images)\n",
    "    if images_per_row is None:\n",
    "        # Make a square grid\n",
    "        grid_size = math.ceil(math.sqrt(n_images))\n",
    "        n_rows = grid_size\n",
    "        n_cols = grid_size\n",
    "    else:\n",
    "        n_cols = images_per_row\n",
    "        n_rows = math.ceil(n_images / n_cols)\n",
    "    \n",
    "    # Get max dimensions\n",
    "    max_width = max(img.size[0] for img in images)\n",
    "    max_height = max(img.size[1] for img in images)\n",
    "    \n",
    "    # Create output image\n",
    "    grid_img = Image.new('RGB', \n",
    "                        (max_width * n_cols, max_height * n_rows),\n",
    "                        color='white')\n",
    "    \n",
    "    # Paste images into grid\n",
    "    for idx, img in enumerate(images):\n",
    "        row = idx // n_cols\n",
    "        col = idx % n_cols\n",
    "        grid_img.paste(img, (col * max_width, row * max_height))\n",
    "    \n",
    "    # Save result\n",
    "    grid_img.save(output_path)\n",
    "    print(f\"Grid image saved to {output_path}\")\n",
    "\n",
    "def create_megaimage(image_dir, images_per_row=None):\n",
    "    output_path = f\"{image_dir}/combined.png\"\n",
    "    # Create a square grid\n",
    "    create_image_grid(image_dir, output_path, images_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "10ab085f-7b99-4e67-abb5-75cd83076683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid image saved to plots/llama_family/combined.png\n",
      "Grid image saved to plots/train_method/combined.png\n"
     ]
    }
   ],
   "source": [
    "#create_megaimage(\"plots/ablations\")\n",
    "create_megaimage(\"plots/llama_family\", images_per_row=2)\n",
    "create_megaimage(\"plots/train_method\", images_per_row=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4602eb4a-5756-4f5c-83be-491af48d21db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid image saved to plots/ablations/combined.png\n"
     ]
    }
   ],
   "source": [
    "create_megaimage(\"plots/ablations\", images_per_row=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3078b80d-2749-430e-a002-0a061d53573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss file found at /n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa/20250109-090124-False/loss.jsonl\n",
      "No loss file found at /n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa_wiki/20250109-092335-False/loss.jsonl\n",
      "No loss file found at /n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/20250105-005619_tags-False/loss.jsonl\n",
      "No loss file found at /n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa_s2d/20250109-094716-False/loss.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    }
   ],
   "source": [
    "analyze_perplexity(model_pairs,\"ablations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fb6d06e6-3222-43b3-b4b5-aab78034da13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-2-7b : QA 6285 post datapoints\n",
      "Llama-2-7b : QA+wiki 6285 post datapoints\n",
      "Llama-2-7b : Q only 6038 post datapoints\n",
      "Llama-2-7b : QA+s2d 0 post datapoints\n",
      "Barplot MMLU Accuracy:\n",
      "Type                  Normal  Pre-OID  Post-OID\n",
      "Model                                          \n",
      "Llama-2-7b : Q only    0.283    0.282     0.275\n",
      "Llama-2-7b : QA        0.283    0.283     0.293\n",
      "Llama-2-7b : QA+wiki   0.283    0.283     0.284\n",
      "After filtering to only create permutation boxplot from common rows: df_post.shape=(2745, 18)\n",
      "Llama-2-7b : QA         915\n",
      "Llama-2-7b : QA+wiki    915\n",
      "Llama-2-7b : Q only     915\n",
      "Name: model, dtype: int64\n",
      "set index {'Llama-2-7b : QA', 'Llama-2-7b : QA+wiki', 'Llama-2-7b : Q only'}\n",
      "Boxplot MMLU Accuracy:\n",
      "                      Pre-OID  Post-OID\n",
      "Llama-2-7b : Q only     0.281     0.259\n",
      "Llama-2-7b : QA         0.281     0.263\n",
      "Llama-2-7b : QA+wiki    0.281     0.267\n"
     ]
    }
   ],
   "source": [
    "analyze_dataset(\"MMLU\",\"ablations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f8fe805d-813e-4a55-a690-d58eefb21ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-2-7b : QA 9741 post datapoints\n",
      "Llama-2-7b : QA+wiki 9741 post datapoints\n",
      "Llama-2-7b : Q only 9741 post datapoints\n",
      "Llama-2-7b : QA+s2d 0 post datapoints\n",
      "Barplot CSQA Accuracy:\n",
      "Type                  Normal  Pre-OID  Post-OID\n",
      "Model                                          \n",
      "Llama-2-7b : Q only    0.256    0.302     0.230\n",
      "Llama-2-7b : QA        0.256    0.302     0.499\n",
      "Llama-2-7b : QA+wiki   0.256    0.302     0.486\n",
      "After filtering to only create permutation boxplot from common rows: df_post.shape=(3000, 18)\n",
      "Llama-2-7b : QA         1000\n",
      "Llama-2-7b : QA+wiki    1000\n",
      "Llama-2-7b : Q only     1000\n",
      "Name: model, dtype: int64\n",
      "set index {'Llama-2-7b : QA', 'Llama-2-7b : QA+wiki', 'Llama-2-7b : Q only'}\n",
      "Boxplot CSQA Accuracy:\n",
      "                      Pre-OID  Post-OID\n",
      "Llama-2-7b : Q only     0.321     0.257\n",
      "Llama-2-7b : QA         0.321     0.503\n",
      "Llama-2-7b : QA+wiki    0.321     0.507\n"
     ]
    }
   ],
   "source": [
    "analyze_dataset(\"CSQA\",\"ablations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b3f463-66f2-4f7d-ad2e-6e3862ce1ca3",
   "metadata": {},
   "source": [
    "# Cached logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "456cc16c-5fe1-4997-9aeb-29b558e5690f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss file found at /n/netscratch/dwork_lab/Lab/katrina/finetuning_sbp/meta-llama/Llama-2-7b-hf/mmlu_quoted_qa_wiki/20250109-092335-False/loss.jsonl\n"
     ]
    }
   ],
   "source": [
    "analyze_perplexity(model_pairs,\"llama_family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a0a25b5e-ae8c-4ad6-9161-c2759972d524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading (30888, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/mmlu_quoted_permutations/meta-llama_Llama-2-7b-chat-hf-50\n",
      "Loading (18414, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/mmlu_quoted/meta-llama_Llama-2-13b-hf-50\n",
      "Loading (13224, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/mmlu_quoted/_n_netscratch_dwork_lab_Lab_katrina_finetuning_sbp_meta-llama_Llama-2-13b-hf_mmlu_quoted_qa_wiki_20250110-002528-False_final_weights-50\n",
      "Loading (30888, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/mmlu_quoted_permutations/meta-llama_Llama-2-13b-hf-50\n",
      "Loading (8796, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/mmlu_quoted/meta-llama_Llama-2-13b-chat-hf-50\n",
      "Loading (13224, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/mmlu_quoted/_n_netscratch_dwork_lab_Lab_katrina_finetuning_sbp_meta-llama_Llama-2-13b-chat-hf_mmlu_quoted_qa_wiki_20250109-235941-False_final_weights-50\n",
      "Loading (30888, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/mmlu_quoted_permutations/meta-llama_Llama-2-13b-chat-hf-50\n",
      "Llama-2-7b : QA+wiki 6285 post datapoints\n",
      "Llama-2-7b-chat : QA+wiki 7885 post datapoints\n",
      "Llama-2-13b : QA+wiki 4408 post datapoints\n",
      "Llama-2-13b-chat : QA+wiki 1843 post datapoints\n",
      "Barplot MMLU Accuracy:\n",
      "Type                        Normal  Pre-OID  Post-OID\n",
      "Model                                                \n",
      "Llama-2-13b : QA+wiki        0.314    0.302     0.259\n",
      "Llama-2-13b-chat : QA+wiki   0.402    0.334     0.288\n",
      "Llama-2-7b : QA+wiki         0.283    0.283     0.284\n",
      "Llama-2-7b-chat : QA+wiki    0.318    0.273     0.287\n",
      "After filtering to only create permutation boxplot from common rows: df_post.shape=(7496, 18)\n",
      "Llama-2-7b : QA+wiki          1924\n",
      "Llama-2-7b-chat : QA+wiki     1924\n",
      "Llama-2-13b : QA+wiki         1824\n",
      "Llama-2-13b-chat : QA+wiki    1824\n",
      "Name: model, dtype: int64\n",
      "Boxplot MMLU Accuracy:\n",
      "                            Pre-OID  Post-OID\n",
      "Llama-2-13b : QA+wiki         0.278     0.174\n",
      "Llama-2-13b-chat : QA+wiki    0.335     0.147\n",
      "Llama-2-7b : QA+wiki          0.281     0.245\n",
      "Llama-2-7b-chat : QA+wiki     0.274     0.246\n"
     ]
    }
   ],
   "source": [
    "analyze_dataset(\"MMLU\",\"llama_family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "30fb92b4-d98b-4ff4-9bad-58bb8d51318b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading (29223, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/csqa_quoted/meta-llama_Llama-2-7b-chat-hf-50\n",
      "Loading (29223, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/csqa_quoted/_n_netscratch_dwork_lab_Lab_katrina_finetuning_sbp_meta-llama_Llama-2-7b-chat-hf_mmlu_quoted_qa_wiki_20250110-001813-False_final_weights-50\n",
      "Loading (67500, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/csqa_quoted_permutations/meta-llama_Llama-2-7b-chat-hf-50\n",
      "Loading (2118, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/csqa_quoted/meta-llama_Llama-2-13b-hf-50\n",
      "Loading (29223, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/csqa_quoted/_n_netscratch_dwork_lab_Lab_katrina_finetuning_sbp_meta-llama_Llama-2-13b-hf_mmlu_quoted_qa_wiki_20250110-002528-False_final_weights-50\n",
      "Loading (67500, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/csqa_quoted_permutations/meta-llama_Llama-2-13b-hf-50\n",
      "Loading (2526, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/csqa_quoted/meta-llama_Llama-2-13b-chat-hf-50\n",
      "Loading (29223, 18) results from /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/csqa_quoted/_n_netscratch_dwork_lab_Lab_katrina_finetuning_sbp_meta-llama_Llama-2-13b-chat-hf_mmlu_quoted_qa_wiki_20250109-235941-False_final_weights-50\n",
      "No results found for /n/netscratch/dwork_lab/Lab/katrina/set-based-prompting-finetuning/results/csqa_quoted_permutations/meta-llama_Llama-2-13b-chat-hf-50\n",
      "Llama-2-7b : QA+wiki 9741 post datapoints\n",
      "Llama-2-7b-chat : QA+wiki 9741 post datapoints\n",
      "Llama-2-13b : QA+wiki 706 post datapoints\n",
      "Llama-2-13b-chat : QA+wiki 842 post datapoints\n",
      "Barplot CSQA Accuracy:\n",
      "Type                        Normal  Pre-OID  Post-OID\n",
      "Model                                                \n",
      "Llama-2-13b : QA+wiki        0.312    0.356     0.476\n",
      "Llama-2-13b-chat : QA+wiki   0.500    0.477     0.456\n",
      "Llama-2-7b : QA+wiki         0.256    0.302     0.486\n",
      "Llama-2-7b-chat : QA+wiki    0.458    0.399     0.538\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'assign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalyze_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCSQA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama_family\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[132], line 33\u001b[0m, in \u001b[0;36manalyze_dataset\u001b[0;34m(dataset_type, suffix)\u001b[0m\n\u001b[1;32m     25\u001b[0m create_comparison_barplot(\n\u001b[1;32m     26\u001b[0m     model_results,\n\u001b[1;32m     27\u001b[0m     dataset_type,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_type\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     suffix,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Create plots with all models\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mcreate_permutation_boxplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 3\u001b[0m, in \u001b[0;36mcreate_permutation_boxplot\u001b[0;34m(model_results, dataset_type, output_name, suffix)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_permutation_boxplot\u001b[39m(model_results, dataset_type, output_name, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create boxplot showing distribution of accuracies across permutations for all models\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([model_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_data_perm\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39massign(model\u001b[38;5;241m=\u001b[39mmodel_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m model_info \u001b[38;5;129;01min\u001b[39;00m model_results])\u001b[38;5;66;03m#.drop_duplicates()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     df_post \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([model_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39massign(model\u001b[38;5;241m=\u001b[39mmodel_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m model_info \u001b[38;5;129;01min\u001b[39;00m model_results])\u001b[38;5;66;03m#.drop_duplicates()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    'response_type', 'model', 'max_new_tokens', 'order_independent_output',\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m       'pad_attention', 'text_output', 'is_correct_answer',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m       'probs', 'meta_label', 'meta_incorrect_answers', 'prompt', 'file_name'\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[130], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_permutation_boxplot\u001b[39m(model_results, dataset_type, output_name, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create boxplot showing distribution of accuracies across permutations for all models\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mmodel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpre_data_perm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m(model\u001b[38;5;241m=\u001b[39mmodel_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m model_info \u001b[38;5;129;01min\u001b[39;00m model_results])\u001b[38;5;66;03m#.drop_duplicates()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     df_post \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([model_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39massign(model\u001b[38;5;241m=\u001b[39mmodel_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m model_info \u001b[38;5;129;01min\u001b[39;00m model_results])\u001b[38;5;66;03m#.drop_duplicates()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    'response_type', 'model', 'max_new_tokens', 'order_independent_output',\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m       'pad_attention', 'text_output', 'is_correct_answer',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m       'probs', 'meta_label', 'meta_incorrect_answers', 'prompt', 'file_name'\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'assign'"
     ]
    }
   ],
   "source": [
    "analyze_dataset(\"CSQA\",\"llama_family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31dbdd-16aa-42e0-90fa-a1bbfefd7e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c8fab-9a20-4cf5-bbbe-ae3c42301bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "45a730ad-e67b-4e35-8839-aea4d360a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-2-7b : QA+wiki 9741 post datapoints\n",
      "Llama-2-7b : QA+wiki : train_csqa 6000 post datapoints\n",
      "Llama-2-7b : QA+wiki (1 epoch) 9741 post datapoints\n",
      "Barplot CSQA Accuracy:\n",
      "Type                               Normal  Pre-OID  Post-OID\n",
      "Model                                                       \n",
      "Llama-2-7b : QA+wiki                0.256    0.302     0.486\n",
      "Llama-2-7b : QA+wiki (1 epoch)      0.256    0.302     0.480\n",
      "Llama-2-7b : QA+wiki : train_csqa   0.258    0.305     0.595\n",
      "After filtering to only create permutation boxplot from common rows: df_post.shape=(3000, 18)\n",
      "Llama-2-7b : QA+wiki                 1000\n",
      "Llama-2-7b : QA+wiki : train_csqa    1000\n",
      "Llama-2-7b : QA+wiki (1 epoch)       1000\n",
      "Name: model, dtype: int64\n",
      "Boxplot CSQA Accuracy:\n",
      "                                   Pre-OID  Post-OID\n",
      "Llama-2-7b : QA+wiki                 0.321     0.507\n",
      "Llama-2-7b : QA+wiki (1 epoch)       0.321     0.501\n",
      "Llama-2-7b : QA+wiki : train_csqa    0.321     0.601\n"
     ]
    }
   ],
   "source": [
    "analyze_dataset(\"CSQA\",\"llama_family\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc32739-6475-44bc-9b67-fc8002e8cd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d946e0-482c-4c25-90b4-79f719be1b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
