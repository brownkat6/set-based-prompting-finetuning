{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d24932-bfb4-45ff-8690-bd4606e9c368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "import order_independent_llm\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1745390f-4b9a-42b0-9818-e2714e44c95b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = order_independent_llm.SplitPrompt.from_json_file(\"../data/csqa_split/csqa_input_01500.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef7d1c6-5cbb-4322-8153-8a9a4c74931f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target ='../results/csqa_split/lmsys_vicuna-7b-v1.5-100/lmsys_vicuna-7b-v1.5-100-csqa_input_01000.jsonl_tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9fecced-334b-4bad-b134-a7abdbd5a9c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p2 = order_independent_llm.filter_prompts(target, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa5c2906-00ef-4695-ada6-264ede0bf874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd9d0118-0817-4f75-b5ce-10207f18ed1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cece4da-de82-49a2-b34a-880ef6863e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(target) as f:\n",
    "    rets = [json.loads(l) for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05728f78-4356-4344-8291-a2392f764aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0].text == rets[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "825f22c9-2686-4b8e-97f1-0ed39ea1bf15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If someone is free from guilt what are they likely to achieve?\\n<|start_2d|>\\n- peaceful sleep<|split_2d|>\\n- freedom from want<|split_2d|>\\n- medal<|split_2d|>\\n- headache<|split_2d|>\\n- new computer<|end_2d|> Answer: '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rets[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e85bda6-a583-46d2-bef6-aab01fe62d53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SplitPrompt(text='If someone is free from guilt what are they likely to achieve?\\n<|start_2d|>\\n- peaceful sleep<|split_2d|>\\n- freedom from want<|split_2d|>\\n- medal<|split_2d|>\\n- headache<|split_2d|>\\n- new computer<|end_2d|> Answer: ', metadata={'label': 'peaceful sleep', 'incorrect_answers': ['freedom from want', 'medal', 'headache', 'new computer']})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099def19-44b6-440c-a8cc-a00f0576ece3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b783fc0-31bf-4d5a-a167-0e040337d6db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5508a16d02b4b5383d870c293a164e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = order_independent_llm.load_model(\"meta-llama/Llama-2-7b-hf\", 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6b98fb-8bdd-4e33-97bc-80e2fc7beb26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ce2e165-3775-4754-9440-b01c4b8f7d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp2 = order_independent_llm.order_independent_query(\n",
    "            prefix=\"test\",\n",
    "            parallel_substrings=['a','b', 'c'],\n",
    "            suffix='\\n1 paragraph summary of the disaster, including damage, invesigation and casualty discussion.\\n',\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=200,\n",
    "            #torch_device='cuda',\n",
    "            is_order_independent=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f928e9c-5b7a-4a2b-b5a2-3e06fcddd531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "998a5adb-c46a-41c4-b7fd-ad27564bfda6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef5f3aa1-97b4-4bf3-9e15-7490f809cc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#resp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "145d421c-2c8d-4274-9e9e-5c9d1b55d4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw = \"\"\"On March 26, 2024, at 1:28 a.m. EDT (05:28 UTC), the main spans and the three nearest northeast approach spans of the Francis Scott Key Bridge across the Patapsco River in the Baltimore metropolitan area of Maryland, United States, collapsed after the container ship Dali struck one of its piers. Two people were rescued from the river; one had no injuries, while the other was transported to a hospital in critical condition. Six members of a maintenance crew working on the roadway were reported missing; three bodies were recovered, and the other three are presumed dead.[1][2]\n",
    "\n",
    "The collapse blocked most shipping to and from the Port of Baltimore. Maryland Governor Wes Moore called the event a \"global crisis\" that had affected more than 8,000 jobs. The economic impact of the waterway's closure has been estimated at $15 million per day.\n",
    "Background\n",
    "Main articles: Francis Scott Key Bridge (Baltimore) and MV Dali\n",
    "Dali's size, though considered large, is less than that of the largest container ship.[3] It is recognized that bigger ships can cause bigger disasters, such as the 1,300-foot vessel in the 2021 Suez Canal obstruction.[3]\n",
    "\n",
    "The Francis Scott Key Bridge was a steel arch-shaped continuous truss bridge, the second-longest in the United States and third-longest in the world.[4] Opened in 1977, the 1.6-mile (2.6 km; 1.4 nmi) bridge ran northeast from Hawkins Point, Baltimore, to Sollers Point in Dundalk in Baltimore County, Maryland. Before being damaged, it carried Interstate 695, a beltway around Baltimore;[5] its four lanes (two in each direction[6]) were used by some 34,000 vehicles each day, including 3,000 trucks, many of which hauled hazardous materials barred from the two harbor tunnels.[7][8]\n",
    "\n",
    "The bridge crossed one of the busiest shipping routes in the United States: the lower Patapsco River, which connects the Port of Baltimore to the Chesapeake Bay and the Atlantic Ocean.[5][7] In 2023, the port handled more than 444,000 passengers and 52.3 million tons of foreign cargo valued at $80 billion.[5] It was the second-largest U.S. port for coal, and had been the leading port for automobiles and light trucks for 13 straight years, handling more than 847,000 vehicles in 2023.[9][10] It employed 15,000 people and indirectly supported 140,000 others,[11] annually helping to generate $3.3 billion in wages and salaries, $2.6 billion in business revenue, and $400 million in state and local tax revenue.[8]\n",
    "\n",
    "MV Dali is a container ship registered in Singapore, and at the time of the allision[a] was operated by Synergy Marine Group[12] and owned by Grace Ocean Private Ltd,[13] both based in Singapore. A Neopanamax vessel completed in 2015, Dali has a length of 980 feet (300 m), a 157-foot (48 m) beam, and a 40-foot (12.2 m) draft.[14] Danish shipping company Maersk chartered Dali upon its delivery.[15] Once in service, Dali had undergone 27 inspections at ports globally,[16][17] including two in 2023: one in June in San Antonio, Chile, where a fuel-pressure gauge was repaired; and the second in September by the U.S. Coast Guard in New York, which found no problems.[18][19][17]\n",
    "\n",
    "In March 2024, Dali was crewed by 20 Indian nationals and one Sri Lankan.[20] The ship traveled from Panama to New York, arriving on March 19,[21] then sailed to the Virginia International Gateway in Portsmouth, Virginia.[22] The ship left Virginia on March 22 and the following day arrived in Baltimore,[23] where it underwent engine maintenance.[24][25]\n",
    "\n",
    "When the bridge was completed in 1977, the largest container ships could hold 2,000 to 3,000 twenty-foot equivalent unit (TEU) containers.[26] After the Panama Canal expansion began to allow the passage of 14,000-TEU vessels in 2016, the Maryland Port Administration installed new cranes and dredged the harbor to enable the port to accommodate the larger ships.[27] At the time of its collision, Dali was loaded nearly to its 10,000-TEU capacity with 4,700 forty-foot containers.[26]\n",
    "\n",
    "In 1980, a ship roughly one-third the size of Dali struck and lightly damaged one of the bridge's piers.[28][29] After the bridge collapsed in 2024, anonymous former agency officials told The Washington Post that the Maryland Transportation Authority (MDTA) did not consider studying the possibility of a collision with a larger ship, and instead spent decades studying how terrorists might attack the bridge after the September 11 attacks or inspecting for structural flaws similar to those that caused the I-35W Mississippi River bridge collapse in 2007.[30]\n",
    "\n",
    "Federal regulations for protecting bridges from ship collisions were updated in 1991 after the Sunshine Skyway Bridge collapse in 1980, but existing bridges were exempted by a grandfather clause, and the Francis Scott Key Bridge piers lacked the level of fender system or island barriers required of newer bridges.[31][32][33] However, engineering experts debate whether such bridge protection systems could have prevented the collapse given Dali's size.[34][35]\n",
    "Collapse\n",
    "Upstream view of the bridge in 2015; Dali hit the fourth pier from left.[36]\n",
    "MV Dali immobilized by the wreckage\n",
    "\n",
    "Dali left the Port of Baltimore at 12:44 a.m. EDT (04:44 UTC) on March 26, 2024,[37] bound for Colombo, Sri Lanka.[38] The ship had two local harbor pilots on board.[36] Following standard operating procedure in Baltimore, tugboats that piloted the ship from its berth were released once the ship was in the channel.[39][40][8] At 1:24 a.m.,[41][42] the ship suffered a \"complete blackout\" and began to drift out of the shipping channel; a backup generator supported electrical systems but did not provide power to the propulsion system.[43] At 1:27 a.m., a mayday call was made from the ship,[42] notifying the Maryland Department of Transportation that the crew had lost propulsion and control of the vessel and that a collision with the bridge was possible.[44]\n",
    "\n",
    "One of the pilots requested that traffic be stopped from crossing the bridge immediately.[45][46][47][48] The ship's lights went out and came on again some moments later; then again went off and returned just before impact as smoke once again began rising from the funnel.[36][49] At the pilot's request, the MDTA Police dispatch asked officers to stop traffic in both directions at 1:27:53 a.m.; outer loop (eastbound/northbound) traffic was stopped at the south side after 20 seconds. Inner loop (westbound/southbound) traffic was stopped at the north side by 1:28:58 a.m., around the time of the collapse.[50] The Maritime and Port Authority of Singapore (MPA) reported that the ship dropped anchor before hitting the bridge, as part of its emergency procedures.[45]\n",
    "\n",
    "At 1:28:45 a.m.,[51][52][53] the ship struck the southwest pier of the central truss arch span, at roughly 8 knots (9.2 mph; 15 km/h).[54] AIS data showed the ship traveling at a speed of 8.7 knots (10.0 mph; 16.1 km/h) at 1:25 a.m. before departing the channel and slowing to 6.8 knots (7.8 mph; 12.6 km/h) by the time of the collision two minutes later.[49][55]\n",
    "\n",
    "Within seconds of the collision, the bridge broke apart in several places,[56] leaving sections protruding from the water and the roadway's approaches cut off.[54] The main span fell onto the ship's bow and a section of it came to rest there.[45][57] The bridge strike and partial collapse were recorded on video.[58][59]\n",
    "\n",
    "Multiple vehicles were on the bridge at the time it collapsed, though initially no one was believed to be inside them.[54] Workers were repairing potholes on the bridge[54] and were in their vehicles on a break at the time of the collapse.[60] A resident living near the bridge recalled being awakened by deep rumbling that shook his residence for several seconds following the collapse, which he said \"felt like an earthquake\".[54]\n",
    "\n",
    "Emergency teams began receiving 911 calls at 1:30 a.m.[45] The Baltimore Police Department was alerted to the collapse at 1:35 a.m. Large rescue and recovery efforts were begun.[58] The Coast Guard deployed boats and a helicopter as part of rescue efforts.[45] Fifty public safety divers in eight teams were dispatched to search for people who fell into the river.[61][45]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4355be1c-8d77-48db-9e42-a3469cb5c61b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "substrings = [\n",
    "    \"\"\"Damage\n",
    "A labelled diagram of the bridge, with Dali's impact point and the collapsed sections illustrated\n",
    "Panoramic photography of the scene as depicted in the diagram.\n",
    "The collapsed portion of the bridge comprises the three spans under the metal truss, and three others to the northeast (left of the images, in Dundalk, Maryland; right is Hawkins Point, Baltimore).[45]\n",
    "2016 photo of the pier struck by the ship\n",
    "Aerial view of the damage\n",
    "\n",
    "The bridge's continuous truss relied on its overall structure to maintain integrity; in engineering terms, it was fracture critical, meaning it had no redundancy against removal of support of any particular part of it.[51] The collision destroyed its southwest main truss pier, causing the south and central spans to collapse, which led to the collapse of a northern span.[58][62] Each failure sequence took seconds, and within 30 seconds the entirety of the trussed spans, and three others, had fallen.[63]\n",
    "\n",
    "The bridge was determined to be fully compliant with the building code[which?] when it collapsed.[45] The bridge had dolphin and fender protection against ship impact, but these protections were insufficient.[32][64][65]\n",
    "\n",
    "Of Dali's 4,700 shipping containers, 13 were damaged in the collision.[19] Two fell into the water, neither of which carried hazardous substances.[66] Dali sustained hull damage above the water line and the ship was impaled by remnants of the bridge superstructure (estimated to be 3,000 to 4,000 tonnes of bridge wreckage),[67][68] which pressed it against the channel floor.[69][70] The ship remained watertight,[67] and the shipping company initially claimed there was no water pollution directly from the ship.[71][72] Authorities installed 2,400 feet (730 m) of water containment booms[73] around the ship after a sheen was detected in the waterway, which was believed to have been produced by 21 US gallons (17 imp gal; 79 L) of oil that leaked from a bow thruster on the ship.[74] On March 27, the National Transportation Safety Board (NTSB) announced an investigation into a hazmat spill resulting from breached containers aboard Dali, including some of the 56 containers that carried about 764 tons of hazardous materials: primarily corrosives, flammable substances (including lithium batteries), and Class 9 materials.\"\"\"\n",
    ",\n",
    "    \"\"\"Casualties\n",
    "\n",
    "NOAA reported a water temperature of 47 °F (8 °C) at the time of the collapse.[54] Two people were rescued from the river: one was in \"very serious\" condition and the other uninjured.[77] One of those rescued was a Mexican national.[13] Six people, all part of the maintenance crew working on the bridge, were reported missing and presumed dead following the suspension of a Coast Guard search.[1][45][78][79][80] One was identified as a Honduran national, two were from Guatemala, and the others were from El Salvador and Mexico.[13][42]\n",
    "\n",
    "Five submerged vehicles, including three passenger vehicles and a transit mixer, were detected using sonar.[81] Emergency services also used drones and infrared technology in the search.[51] The bodies of two of the maintenance crew were recovered from inside a red pickup truck: a 35-year-old Mexican national and a 26-year-old Guatemalan national.[1][82] They were found at a depth of 25 feet (7.6 m) below the mid-section of the bridge.[83] The search was suspended based on the condition of the debris and risk of further collapse.[1] On April 5, the body of a 38-year-old Honduran national was recovered from a submerged vehicle. A 35-year-old Guatemalan national, a 49-year-old Salvadoran national, and Mexican national remain among the missing.[84]\n",
    "\n",
    "Occupational Safety and Health Administration regulations require that construction companies keep skiffs available at construction sites over waterways. Coast Guard officials said they were unaware of whether the company that employed the highway workers had one available, and satellite imagery at the time of the bridge collapse does not appear to show one present at the bridge. The company declined to respond to press inquiries about whether a boat was available.[85]\n",
    "\n",
    "Dali's crew and the two pilots were accounted for and did not sustain any serious injuries.[71] One crew member was slightly injured and required some stitches.[66] Groups such as the Baltimore International Seafarers' Center made efforts to support the crew members as they remained on the boat,[86] including providing them with Wi-Fi hotspots.\"\"\",\n",
    "    \"\"\"Investigation\n",
    "External videos\n",
    "video icon Press conference with NTSB Chair Jennifer Homendy, March 26, 2024, C-SPAN\n",
    "Three people in FBI uniforms are on a boat. They are closely inspecting the mangled remains of bridge struts poking out of the water in front of them.\n",
    "An evidence response team from the FBI examines a segment of the bridge several hours after the collapse.\n",
    "Folding tables are arranged in rows and a square in a large presentation room in a police station. People in various uniforms representing many agencies are seated at the tables and focused on their laptops. The center of the room has a table piled with boxes of pizza, salads, and other provisions.\n",
    "Officials coordinating response and rescue efforts at the Maryland Transportation Authority headquarters on the day of the collapse\n",
    "\n",
    "The NTSB began an investigation and sent a team to the site.[88] The agency is expected to release a preliminary report two to four weeks after the collapse, and later issue urgent safety recommendations, while its investigation could take between 12 and 24 months.[1][89] The Federal Bureau of Investigation (FBI) was also deployed to the scene, but said that terrorism was not suspected in the incident.[90][54] On March 27, a Unified Command Joint Information Center was established to coordinate the investigation and salvage. The command includes team members from the U.S. Coast Guard, Maryland Department of the Environment, MDTA, Maryland State Police, and Synergy Marine, as the primary stakeholders.[91]\n",
    "\n",
    "As the flag state, Singapore's Transport Safety Investigation Bureau (TSIB) and the MPA sent personnel to Baltimore to help in investigations. The MPA said it offered support to the NTSB and the Office of Marine Safety.[92]\n",
    "\n",
    "NTSB personnel boarded the ship late on March 26 and obtained the voyage data recorder (VDR), which would help investigators develop a timeline of events leading up to the collision.[93][94] Several possible factors were being considered, including the possibility that contaminated fuel or an improper grade of fuel had caused the loss of the ship's power.[95][96][97] At a Senate Commerce Committee hearing on April 10, NTSB Chair Jennifer Homendy stated that the agency was gathering data about the ship's electrical system and examining its circuit breakers with the assistance of Hyundai Heavy Industries, Dali's shipbuilder.\"\"\"\n",
    "\"\"\"Aftermath\n",
    "\n",
    "The debris from the collapse has blocked maritime access to virtually the entirety of the Port of Baltimore; nearly 30 ships had signaled the port as their destination, and more than 40 were trapped.[106] Only one part of the Port of Baltimore was unaffected: the Tradepoint Atlantic marine terminal at Sparrows Point, on the seaward side of the Key Bridge.[107] Tradepoint Atlantic said on April 3 that it began preparing for an influx of redirected ships and estimated that it would unload and process 10,000 vehicles over the next 15 days.[108]\n",
    "\n",
    "Maryland governor Wes Moore declared a state of emergency shortly thereafter,[45] and Maryland Secretary of Transportation Paul Wiedefeld ordered the suspension of all shipping to and from the Port of Baltimore[109] until further notice; trucking facilities remained operational.[54] At 4:15 a.m., the Federal Aviation Administration imposed a 5-nautical-mile (5.8 mi; 9.3 km) temporary flight restriction around the incident site.[110] Maersk, which chartered the vessel,[111] saw the price of its shares decline by about 2% when trading opened at Nasdaq Copenhagen on March 26.[112]\n",
    "Salvage\n",
    "Chesapeake 1000 on-site\n",
    "\n",
    "The U.S. Army Corps of Engineers (USACE) is taking the lead in removing the fallen portions of the bridge. The U.S. Navy is planning to remove the submerged portions using barges with heavy-lift cranes, including the \"largest crane ship on the East Coast\": the Chesapeake 1000 of the Donjon Marine Co., able to lift 1,000 short tons (890 long tons; 910 t).[113][114] The designated salvor is Resolve Marine.[69] Thirty-two USACE personnel and 38 Navy contractors were deployed to the scene.[73] More than 1,100 engineering specialists were[needs update] to join them.[115] Seven floating cranes, ten tugboats, nine barges, eight salvage vessels, and five Coast Guard boats were deployed around the bridge.[116]\n",
    "\n",
    "On March 30, engineers began removing the first piece of the bridge from the river.[117][118][119] On April 1, the Coast Guard opened a temporary passage for commercial work vessels involved in recovery and clearing efforts, with a controlling depth of 11 feet (3.4 m), a horizontal clearance of 264 feet (80 m) and a vertical clearance of 96 feet (29 m),[120] and was approving ships' passage case by case.[121] The next day, the first work vessel used the alternate channel: a tugboat pushing a fuel barge to Dover Air Force Base in Delaware. A second channel was opened the next day, as work continued on a third channel.[122][123] On April 7, salvage crews started removing containers from Dali.\"\"\",\n",
    "\"\"\"Responses\n",
    "President Biden is briefed on the collapse\n",
    "External videos\n",
    "video icon Remarks by U.S. president Joe Biden on the bridge collapse, March 26, 2024, C-SPAN\n",
    "\n",
    "President Joe Biden was briefed on the disaster within hours of the collision.[54] U.S. Secretary of Transportation Pete Buttigieg contacted Maryland governor Wes Moore and Baltimore mayor Brandon Scott to offer his department's support.[161] Moore addressed the families of the victims in Spanish, saying, \"Estamos contigo, ahora y siempre [we are with you, now and always]\".[89] Maryland Center for History and Culture vice president David Belew said, \"Our harbor, port and many families are fundamentally changed\" by the disaster.[162] On March 27, Moore and Biden thanked Dali's crew for transmitting the mayday call warning of the ship's power failure and the impending collision.[163][164] On March 28, three officers of the MDTA were recognized at the opening game of the Baltimore Orioles for their role in stopping traffic before the bridge collapsed.[74]\n",
    "President Biden joins Governor Moore and local officials to speak near the bridge\n",
    "\n",
    "Biden visited the site on April 5; he surveyed the wreckage from Marine One and was later briefed by officials from the local government, the Coast Guard and USACE. He pledged the support of the federal government for a bridge replacement and the recovery effort \"every step of the way\", adding that \"the nation has your back\". He also met with families of the victims.[165][166]\n",
    "\n",
    "The Mexican embassy in the U.S. is providing consular assistance to the families, with a dedicated phone line for affected Mexican nationals.[167] Mexican president Andrés Manuel López Obrador said the disaster highlighted the contribution of migrants to the US economy and \"demonstrates that migrants go out and do risky jobs at midnight\".[168] Rafael Laveaga Rendón, head of the consular section, travelled to Baltimore to help the workers’ families.[169] It has been confirmed that one of the rescued was from Michoacán, while the two Mexican nationals who are still missing are from Michoacán and Veracruz.\"\"\"\n",
    "\"\"\"Replacement bridge\n",
    "\n",
    "In an address on March 26, Biden said that he would ask Congress to fund a replacement bridge.[170] On March 28, the federal government released an initial $60 million in emergency aid under the Emergency Relief (ER) Program of the Federal Highway Administration (FHWA) that is subsidized by the Highway Trust Fund.[74][171] Buttigieg also urged Congress to provide funding for a replacement bridge.[172] Senate Minority Leader Mitch McConnell said that it was the federal government's responsibility to absorb the costs.[173]\n",
    "\n",
    "The House Freedom Caucus issued a statement listing demands for their support of funding for a replacement bridge, including that the federal government seek maximum liability from the shipping companies upfront.[174][175] While vowing to hold those responsible for the bridge collapse accountable, Maryland Senator Ben Cardin argued against waiting for related litigation to be resolved and insurance claims to be approved, saying \"We're not going to delay opening our channel or rebuilding our bridge with the lengthy process that may take\", with which Buttigieg agreed.[166][176] On April 8, Moore stated that he would meet with members of Congress during the following week to discuss potential support for funding a replacement bridge.[177]\n",
    "\n",
    "While some engineering professors suggested that replacing the bridge could take as long as 10 years and cost at least $350 million,[70][178][179] a report issued by the Congressional Research Service (CRS) noted that replacement bridges can qualify for a Categorical Exclusion (CE) under the National Environmental Policy Act to accelerate regulatory review and project delivery and that the I-35W Saint Anthony Falls Bridge that replaced the I-35W Mississippi River bridge (which received a CE) was completed in 11 months, and repairs to the Sunshine Skyway Bridge took five years to complete.[171]\n",
    "\n",
    "The CRS report notes further that a replacement bridge could be eligible for additional funding from the FHWA ER Program that could cover 80% of the project cost since the bridge was a state highway, 90% of the project cost if the expenses cause the state government to exceed its federal-aid highway program funds for the fiscal year, or 100% of the project cost if Congress makes an exception for the project from the ER Program rules (which Congress did for the I-35W Saint Anthony Falls Bridge), while any state funds received from an approved insurance claim would offset funding awarded from the ER Program.[171] On April 9, the Maryland congressional delegation announced that they would introduce a bill to make an exception to the ER Program rules for a replacement bridge.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cc4947b-4ff4-47ee-9945-9943bcdda41a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 3            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  27148 MiB |  27148 MiB |  87245 MiB |  60097 MiB |\n",
      "|       from large pool |  27097 MiB |  27097 MiB |  76337 MiB |  49239 MiB |\n",
      "|       from small pool |     50 MiB |    156 MiB |  10908 MiB |  10858 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  27148 MiB |  27148 MiB |  87245 MiB |  60097 MiB |\n",
      "|       from large pool |  27097 MiB |  27097 MiB |  76337 MiB |  49239 MiB |\n",
      "|       from small pool |     50 MiB |    156 MiB |  10908 MiB |  10858 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  27145 MiB |  27145 MiB |  86237 MiB |  59092 MiB |\n",
      "|       from large pool |  27095 MiB |  27095 MiB |  75373 MiB |  48278 MiB |\n",
      "|       from small pool |     50 MiB |    156 MiB |  10863 MiB |  10813 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  27282 MiB |  27282 MiB |  28012 MiB | 747520 KiB |\n",
      "|       from large pool |  27228 MiB |  27228 MiB |  27724 MiB | 507904 KiB |\n",
      "|       from small pool |     54 MiB |    170 MiB |    288 MiB | 239616 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 137163 KiB | 188429 KiB |  81402 MiB |  81268 MiB |\n",
      "|       from large pool | 133185 KiB | 176887 KiB |  69564 MiB |  69434 MiB |\n",
      "|       from small pool |   3978 KiB |  46258 KiB |  11838 MiB |  11834 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     971    |    1010    |  506559    |  505588    |\n",
      "|       from large pool |     431    |     485    |   21559    |   21128    |\n",
      "|       from small pool |     540    |     542    |  485000    |  484460    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     971    |    1010    |  506559    |  505588    |\n",
      "|       from large pool |     431    |     485    |   21559    |   21128    |\n",
      "|       from small pool |     540    |     542    |  485000    |  484460    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     296    |     362    |     437    |     141    |\n",
      "|       from large pool |     269    |     277    |     293    |      24    |\n",
      "|       from small pool |      27    |      85    |     144    |     117    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     105    |     150    |  182280    |  182175    |\n",
      "|       from large pool |      37    |      44    |    4492    |    4455    |\n",
      "|       from small pool |      68    |     133    |  177788    |  177720    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d5d00bd-c3db-42a6-a8d5-f20da41b9f31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 79.15 GiB of which 16.00 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 26.51 GiB is allocated by PyTorch, and 133.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resp2 \u001b[38;5;241m=\u001b[39m \u001b[43morder_independent_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder_independent_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel_substrings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubstrings\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m1 paragraph summary of the disaster, including damage, invesigation and casualty discussion.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#torch_device='cuda',\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_order_independent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/n/holylabs/LABS/dwork_lab/Lab/reidmcy/projects/triple_queries_order_dependence/order_independent_llm/input_processing.py:411\u001b[0m, in \u001b[0;36morder_independent_query\u001b[0;34m(prefix, parallel_substrings, suffix, model, tokenizer, max_new_tokens, is_order_independent, modify_model, edit_position, edit_attention, pad_attention, metadata)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modify_model:\n\u001b[1;32m    410\u001b[0m     model \u001b[38;5;241m=\u001b[39m get_2D_attention_accepting_model(model)\n\u001b[0;32m--> 411\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokAll\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# remove from GPU\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/transformers/generation/utils.py:1479\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1463\u001b[0m         input_ids,\n\u001b[1;32m   1464\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1476\u001b[0m     )\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1478\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/transformers/generation/utils.py:2316\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2313\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2316\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2319\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2320\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2324\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1188\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1185\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1188\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1032\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_flash_attention_2:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# 2d mask is passed through the layers\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cuda_11/lib/python3.11/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 79.15 GiB of which 16.00 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 26.51 GiB is allocated by PyTorch, and 133.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "resp2 = order_independent_llm.order_independent_query(\n",
    "            prefix=raw,\n",
    "            parallel_substrings=substrings,\n",
    "            suffix='\\n1 paragraph summary of the disaster, including damage, invesigation and casualty discussion.\\n',\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=200,\n",
    "            #torch_device='cuda',\n",
    "            is_order_independent=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555114d4-d80e-415e-b7e4-43ff305ba360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp2.raw_output.scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6755d-00dd-454a-a12b-7e54d49bec80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp3 = order_independent_llm.order_independent_query(\n",
    "            prefix=raw,\n",
    "            parallel_substrings=substrings[::-1],\n",
    "            suffix='\\n1 paragraph summary of the disaster, including damage, invesigation and casualty discussion.\\n',\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=200,\n",
    "            torch_device='cuda',\n",
    "            is_order_independent=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64836c49-2933-4aab-a34e-3ccac1f283d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp3.raw_output.scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1954f5-d83f-4332-98d2-89eaf60bfc86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp2.text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb8b520d-35c3-46fc-af31-9afc16bbc2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Francis Scott Key Bridge was a steel arch-shaped continuous truss bridge, the second-longest in the United States and third-longest in the world.[4] Opened in 1977, the 1.6-mile (2.6 km; 1.4 nmi) bridge ran northeast from Hawkins Point, Baltimore, to Sollers Point in Dundalk in Baltimore County, Maryland. Before being damaged, it carried Interstate 695, a beltway around Baltimore;[5] its four lanes (two in each direction[6]) were used by some 34,000 vehicles each day, including 3,000 trucks, many of which hauled hazardous materials barred from the two harbor tunnels.[7][8]\\n\\nThe bridge crossed one of the busiest shipping routes in the United States: the lower Patapsco River, which connects the Port of Baltimore to the Chesapeake Bay and the Atlantic Ocean.[5][7] In 2023, the port handled more than'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp3.text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1ede625-7540-44f2-90b0-21b49242266c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.8651, 10.2671,  9.5885,  ..., -6.6185, -6.6184, -6.6184]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 0.0641,  3.1897,  1.8972,  ..., -9.0719, -9.0718, -9.0718]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 8.3073,  7.6090,  6.0796,  ..., -6.5057, -6.5057, -6.5057]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 8.1259,  8.8570,  5.0670,  ..., -5.4954, -5.4956, -5.4955]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 7.3216,  7.5358,  6.6497,  ..., -7.1498, -7.1497, -7.1498]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 5.3539,  6.9131,  6.4476,  ..., -4.2093, -4.2093, -4.2093]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 3.5979,  5.3973,  2.7254,  ..., -5.8673, -5.8673, -5.8673]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 5.1628,  6.1174,  4.5418,  ..., -6.4156, -6.4155, -6.4155]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 5.8885,  6.3962,  4.3591,  ..., -7.5208, -7.5208, -7.5208]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 5.8219,  6.7970,  5.4301,  ..., -4.2245, -4.2245, -4.2245]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp3.raw_output.scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1bb83-ac0d-4790-b9cb-ae9cccfbe012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8987998d96cb4a6eac6f7687ff43347a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = order_independent_llm.load_model(\"meta-llama/Llama-2-70b-hf\", 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1450655d-7651-4ceb-8085-9d814da60a94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9ea591b6c74c0c98493a3f7e526e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9ff7e9ddc34bf68e1e916bc8d6267d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a01f8efe6c84c2bac14ea7de2b99efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = order_independent_llm.load_model(\"meta-llama/Llama-2-70b-chat-hf\", 'cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
